{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP and Severity Similarity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLJ6gNzESEHMDjUdQzJwc3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ab2207/Academic-Projects-Data-Analytics/blob/main/Severity%20and%20similarity%20checker%20in%20tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZZRSVuD7-5y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9767147e-77fe-4424-c83c-3d869bdd93fb"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEbABkV28-i9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b56eab57-1510-486e-ce0f-807333e7f04e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import string \n",
        "string.punctuation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')\n",
        "import re\n",
        "import unicodedata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_9Sf9JX9C8n"
      },
      "source": [
        "train_data = pd.read_csv(\"/content/gdrive/My Drive/train.csv\")\n",
        "test_data = pd.read_csv(\"/content/gdrive/My Drive/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IQI369A8KYT"
      },
      "source": [
        "def tweets_cleaning(text):\n",
        "    lowercase = text.lower()\n",
        "    punc_removal = [char for char in lowercase if char not in string.punctuation]\n",
        "    punc_removal_joined = ''.join(punc_removal)\n",
        "    url_removal = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', punc_removal_joined, flags=re.MULTILINE)\n",
        "    emoji_removal = url_removal.encode('ascii', 'ignore').decode('ascii')\n",
        "    stopwords_removal = [word for word in emoji_removal.split() if word not in stopwords.words('english')]\n",
        "    return stopwords_removal\n",
        "\n",
        "train_data['cleaned_text'] = train_data['text'].apply(tweets_cleaning)\n",
        "test_data['cleaned_text'] = test_data['text'].apply(tweets_cleaning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSLRtz2C9J4a"
      },
      "source": [
        "x_train = train_data['cleaned_text']\n",
        "y_train = train_data['label']\n",
        "x_test = test_data['cleaned_text']\n",
        "y_test = test_data['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH9kRGkCFnjS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c66923a9-699b-483e-9c53-ab06b2ebc923"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [police, investigating, suspicious, fire, hunt...\n",
              "1        [rt, andrewconstance, reminder, road, users, p...\n",
              "2        [rt, mattkeanmp, regional, jobs, cheaper, powe...\n",
              "3        [rt, jackbegbie, love, tram, content, andor, d...\n",
              "4        [rt, pfa809, wildfire, risk, rises, plan, plac...\n",
              "                               ...                        \n",
              "14995                      [rt, lesshumbleteej, lmaoooooo]\n",
              "14996    [rt, daandina, es, desolador, que, en, madrid,...\n",
              "14997    [rt, lsaazul, los, jovconfuturo, del, centro, ...\n",
              "14998    [rt, gwendavies12345, yvonner100, rishisunak, ...\n",
              "14999    [rt, accessemployers, makes, persons, disabili...\n",
              "Name: cleaned_text, Length: 15000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGarStTr9J7y"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, Dense, LSTM, Flatten, GlobalAveragePooling1D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNV-TNi89J-Y"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token = \"<OOV>\")\n",
        "tokenizer.fit_on_texts(x_train.values)\n",
        "\n",
        "word_index = tokenizer.word_index #word tokenizing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEAJ3t_Y9KLM"
      },
      "source": [
        "vocab_size = len(word_index) + 1\n",
        "pad_pos = \"post\"\n",
        "trunc_pos = \"post\"\n",
        "epochs = 30\n",
        "drop_rate = 0.1\n",
        "input_len = 27\n",
        "loss = \"binary_crossentropy\"\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
        "batch_size = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aGd1nl48Kfw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a9e17ffc-2f09-4a33-e7e9-12fb90d9eb92"
      },
      "source": [
        "train_sentence_tokenizing = tokenizer.texts_to_sequences(x_train.values)\n",
        "train_padding = pad_sequences(train_sentence_tokenizing, padding = pad_pos, truncating = trunc_pos)\n",
        "train_padding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  582,  5495, 13634, ...,     0,     0,     0],\n",
              "       [    2,  2115,   882, ...,     0,     0,     0],\n",
              "       [    2,  8864,  4609, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [    2,  3017,    27, ...,     0,     0,     0],\n",
              "       [    2, 13062, 13063, ...,     0,     0,     0],\n",
              "       [    2, 35315,   331, ...,     0,     0,     0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR3ZYa5e8Kiy"
      },
      "source": [
        "test_token = Tokenizer(oov_token = \"<OOV>\")\n",
        "test_token.fit_on_texts(x_test.values)\n",
        "\n",
        "test_word_index = test_token.word_index\n",
        "test_sentence_tokenizing = test_token.texts_to_sequences(x_test.values)\n",
        "test_padding = pad_sequences(test_sentence_tokenizing, padding = pad_pos, truncating = trunc_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXvlOPIeHYfD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "37163236-9b78-4246-dbd3-cf436f8ae236"
      },
      "source": [
        "test_padding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2, 1595,  280, ...,    0,    0,    0],\n",
              "       [   2, 4414, 4415, ...,    0,    0,    0],\n",
              "       [1452,  134,  749, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [   2,  626,  627, ...,    0,    0,    0],\n",
              "       [   2,  626,  627, ...,    0,    0,    0],\n",
              "       [   2,  189,  170, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brveYlnu8KlR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fb363148-e89c-4de0-8b18-62ae0accb523"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_review(train_padding[1]))\n",
        "print(train_sentence_tokenizing[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rt andrewconstance reminder road users please safe around light rail almost every day seeing risky dangerous b ? ? ? ? ? ? ? ? ? ?\n",
            "[2, 2115, 882, 1387, 1590, 141, 101, 385, 926, 1945, 748, 151, 71, 962, 2116, 1119, 333]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4ZMBkc_8Knw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d26e3ee4-daa8-4eb3-a142-2d6ece882880"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 128, input_length = train_padding.shape[1]))\n",
        "#model.add(GlobalAveragePooling1D())\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "#model.add(Dense(32, activation = 'sigmoid'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss = loss, optimizer = optimizer, metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 27, 128)           4520448   \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 256)               263168    \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 4,783,873\n",
            "Trainable params: 4,783,873\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87XCeSjj8Kqq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "630cbeb1-e1a8-4705-aa7f-ac5e7deb5177"
      },
      "source": [
        "model.fit(train_padding, y_train, epochs = epochs, batch_size = batch_size, validation_data = (test_padding, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "15/15 [==============================] - 2s 121ms/step - loss: 0.7790 - accuracy: 0.6185 - val_loss: 0.7669 - val_accuracy: 0.5136\n",
            "Epoch 2/30\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 0.3929 - accuracy: 0.8304 - val_loss: 0.9685 - val_accuracy: 0.5155\n",
            "Epoch 3/30\n",
            "15/15 [==============================] - 1s 77ms/step - loss: 0.1212 - accuracy: 0.9569 - val_loss: 1.4107 - val_accuracy: 0.5538\n",
            "Epoch 4/30\n",
            "15/15 [==============================] - 1s 72ms/step - loss: 0.0262 - accuracy: 0.9933 - val_loss: 1.7715 - val_accuracy: 0.5626\n",
            "Epoch 5/30\n",
            "15/15 [==============================] - 1s 72ms/step - loss: 0.0102 - accuracy: 0.9976 - val_loss: 2.0576 - val_accuracy: 0.5598\n",
            "Epoch 6/30\n",
            "15/15 [==============================] - 1s 72ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 2.2886 - val_accuracy: 0.5451\n",
            "Epoch 7/30\n",
            "15/15 [==============================] - 1s 74ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 2.3640 - val_accuracy: 0.5469\n",
            "Epoch 8/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 2.3581 - val_accuracy: 0.5441\n",
            "Epoch 9/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 2.5537 - val_accuracy: 0.5426\n",
            "Epoch 10/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 2.6365 - val_accuracy: 0.5426\n",
            "Epoch 11/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 2.7094 - val_accuracy: 0.5413\n",
            "Epoch 12/30\n",
            "15/15 [==============================] - 1s 70ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 2.7905 - val_accuracy: 0.5401\n",
            "Epoch 13/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 2.9099 - val_accuracy: 0.5407\n",
            "Epoch 14/30\n",
            "15/15 [==============================] - 1s 70ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 2.9412 - val_accuracy: 0.5415\n",
            "Epoch 15/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 2.9929 - val_accuracy: 0.5395\n",
            "Epoch 16/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 3.0697 - val_accuracy: 0.5442\n",
            "Epoch 17/30\n",
            "15/15 [==============================] - 1s 70ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 3.0399 - val_accuracy: 0.5495\n",
            "Epoch 18/30\n",
            "15/15 [==============================] - 1s 72ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 3.1589 - val_accuracy: 0.5522\n",
            "Epoch 19/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0050 - accuracy: 0.9981 - val_loss: 3.2072 - val_accuracy: 0.5172\n",
            "Epoch 20/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 2.5671 - val_accuracy: 0.5446\n",
            "Epoch 21/30\n",
            "15/15 [==============================] - 1s 73ms/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 2.5688 - val_accuracy: 0.5455\n",
            "Epoch 22/30\n",
            "15/15 [==============================] - 1s 73ms/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 2.7005 - val_accuracy: 0.5427\n",
            "Epoch 23/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 2.9315 - val_accuracy: 0.5506\n",
            "Epoch 24/30\n",
            "15/15 [==============================] - 1s 72ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 3.0724 - val_accuracy: 0.5311\n",
            "Epoch 25/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 2.9444 - val_accuracy: 0.5448\n",
            "Epoch 26/30\n",
            "15/15 [==============================] - 1s 70ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 3.1079 - val_accuracy: 0.5342\n",
            "Epoch 27/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 3.2648 - val_accuracy: 0.5321\n",
            "Epoch 28/30\n",
            "15/15 [==============================] - 1s 70ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 3.2932 - val_accuracy: 0.5354\n",
            "Epoch 29/30\n",
            "15/15 [==============================] - 1s 70ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 3.2847 - val_accuracy: 0.5354\n",
            "Epoch 30/30\n",
            "15/15 [==============================] - 1s 71ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 3.4163 - val_accuracy: 0.5323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc88b73a9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihGgr6Qm8KtU"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhhfAkDdOzNe"
      },
      "source": [
        "with open('/content/gdrive/My Drive/word_list.txt', 'r') as word:\n",
        "    bow = word.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdkKfxOqOzUX"
      },
      "source": [
        "with open('/content/gdrive/My Drive/tokenized_words.txt', 'w') as words:\n",
        "    for word in word_index:\n",
        "        words.write(word)  \n",
        "\n",
        "with open('/content/gdrive/My Drive/tokenized_words.txt', 'r') as tok_words:\n",
        "    tokenized_words = tok_words.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjlVO61JOzZg"
      },
      "source": [
        "bag_of_words = nlp(bow)\n",
        "tweet_words = nlp(tokenized_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce0trgPWPTEY"
      },
      "source": [
        "similarity = round(tweet_words.similarity(bag_of_words)*100,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWmOjV2e-Xh6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBYgoJ5G-XtW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMglAXfy-Xfs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}